{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOTr Waze Analysis (November 2018)\n",
    "Code and supplementary documentation used to generate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import calendar\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a local mmda_feed.csv file for analysis\n",
    "I create another version using the master MMDA database as a source. I drop the `ds` column with `df.dropna` then I save it to CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Panji\\Documents\\Python Scripts\\Non-Jupyter Py Scripts\\MMDA Tweet2Map\\data_mmda_traffic_alerts.csv')\n",
    "df['ds'] = df['Date'] + ' ' + df['Time']\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df = df[['ds', 'Location', 'Latitude', 'Longitude', 'Direction', 'Type', 'Lanes Blocked', 'Involved', 'Tweet', 'Source']]\n",
    "df.dropna(subset=['ds'], inplace=True)\n",
    "df.to_csv(r'C:\\Users\\Panji\\Documents\\Python Scripts\\Non-Jupyter Py Scripts\\DOTr\\mmda_feed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Create bins to separate data\n",
    "### Create function to analyze the CSV files\n",
    "Generate a CSV file for each hour of each day. The function `split_by_date` will go through the entire raw Waze CSV file. The size of the hour bin is dependent on the `bin_hour_size` argument. The timeframe is seen below as the `date_start` and `date_end` variable. These variables are what the while loop will depend on and the while loop will stop once it reaches the date stated in the `date_end` variable.\n",
    "\n",
    "Each loop will save an output in a .CSV file whose filename will be determined by the `dfFilename` variable. This formula is:\n",
    "\n",
    "` (Year) + (Month) + (Day) + ' ' + ('Time Lower Bound) + ('Time Upper Bound') + (File Extension .csv)`            \n",
    "\n",
    "Where time bounds are in 24 hour format. For example, 20180401_0001 will read as 4 April 2018 00:00 - 01:00 AM.\n",
    "\n",
    "### Functions for 24 hour bins vs non-24 hour bins\n",
    "The `split_by_date` function does not function properly when a bin size of 24 is inputted. Thus a separate function based on the original code was made, `split_by_date_24hourbin`.\n",
    "\n",
    "At the moment, the database that is labelled as `MMDA` serves as the test database while the `WAZE` database serves as the database that the test data will be tested against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_date(df,database, date_start, date_end, bin_hour_size=1, print_statements=False):\n",
    "    \"\"\"\n",
    "    df: Insert a Pandas dataframe\n",
    "    database: 'MMDA' or 'WAZE'\n",
    "    date format for date inputs: mm.dd.yyyy\n",
    "    bin_hour_size: Size (in hours) of the bin\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    import time\n",
    "    \n",
    "    # Check database format\n",
    "    if database == 'MMDA':\n",
    "        databaseOutput = 'mmda_hourly'\n",
    "    elif database == 'WAZE':\n",
    "        databaseOutput = 'waze_hourly'\n",
    "    else:\n",
    "        raise ValueError(f'Invalid database argument: {database}')\n",
    "    \n",
    "    # Clean output folder\n",
    "    folder = 'data\\\\' + databaseOutput\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    dateEnd = date_end\n",
    "    dateStart = date_start\n",
    "    # Get variables for the end dates\n",
    "    dateEndMonth = int(dateEnd.split('.')[0])\n",
    "    dateEndDay = int(dateEnd.split('.')[1])\n",
    "    dateEndYear = int(dateEnd.split('.')[2])\n",
    "    # Get variables for the start dates\n",
    "    dateMonth = int(dateStart.split('.')[0])\n",
    "    dateDay = int(dateStart.split('.')[1])\n",
    "    dateYear = int(dateStart.split('.')[2])\n",
    "\n",
    "    # Initial variables for loop\n",
    "    runLoop = True\n",
    "    timeLowerBound = 0\n",
    "    timeUpperBound = 0 + bin_hour_size\n",
    "    if print_statements == True:\n",
    "        print('=================================')\n",
    "        print(f'START {database} ANALYSIS')\n",
    "        print('=================================')\n",
    "    \n",
    "    while runLoop == True:\n",
    "\n",
    "        # print(f'{dateYear}{dateMonth}{dateDay}')\n",
    "        # print(f'{timeLowerBound}{timeUpperBound}')\n",
    "        if timeUpperBound < 25:\n",
    "            # 24 hours in a day. If it detects the time variables reach 25, then it must go to the ELSE statement\n",
    "            df_selection = df[((df['Timestamp'].dt.month == dateMonth) & (df['Timestamp'].dt.day == dateDay)) & \n",
    "               ((df['Timestamp'].dt.hour >= timeLowerBound) & (df['Timestamp'].dt.hour < timeUpperBound))]\n",
    "        else:\n",
    "            # Restart the time and move onto the next day\n",
    "            if print_statements == True:\n",
    "                print(f'Current Process: {dateMonth}.{dateDay}.{dateYear}')\n",
    "            dateDay += 1\n",
    "            timeLowerBound = 0\n",
    "            timeUpperBound = timeLowerBound + bin_hour_size\n",
    "            \n",
    "        \n",
    "        # Create filename for analysis in GIS software\n",
    "        # Add 0 padding to day numbers with single digit\n",
    "        if len(str(dateDay)) == 1:\n",
    "            dateDayFile = '0' + str(dateDay)\n",
    "        else:\n",
    "            dateDayFile = dateDay\n",
    "        if len(str(dateMonth)) == 1:\n",
    "            dateMonthFile = '0' + str(dateMonth)\n",
    "        else:\n",
    "            dateMonthFile = dateMonth\n",
    "        if len(str(timeLowerBound)) == 1:\n",
    "            timeLowerBoundFile = '0' + str(timeLowerBound)\n",
    "        else:\n",
    "            timeLowerBoundFile = timeLowerBound\n",
    "        if len(str(timeUpperBound)) == 1:\n",
    "            timeUpperBoundFile = '0' + str(timeUpperBound)\n",
    "        else:\n",
    "            timeUpperBoundFile = timeUpperBound\n",
    "        \n",
    "        # Create appropriate filename, then save it\n",
    "        dfFilename = (str(dateYear) + str(dateMonthFile) + str(dateDayFile) + '_' \n",
    "                      + str(timeLowerBoundFile) + str(timeUpperBoundFile) + '.csv')        \n",
    "        df_selection.to_csv('data\\\\' + databaseOutput + '\\\\' + dfFilename, index=False)\n",
    "\n",
    "        # Check if there are still days in the month. If not, proceed to next month\n",
    "        if int(dateDay) > calendar.monthrange(dateYear, dateMonth)[1]:\n",
    "            dateDay = 1\n",
    "            dateMonth += 1\n",
    "            if print_statements == True:\n",
    "                print('--- NEW MONTH START')\n",
    "            # print(f'Current Process: {dateMonth}.{dateDay}.{dateYear}')\n",
    "        \n",
    "        timeLowerBound += 1\n",
    "        timeUpperBound += 1\n",
    "\n",
    "        # Check to see if we've reached the end of the database\n",
    "        if ((dateMonth == dateEndMonth) & (dateDay == dateEndDay) & (dateYear == dateEndYear)):\n",
    "            break\n",
    "    \n",
    "    return print(f'{database} analysis done!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for the 24 hour bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_date_24hourbin(df,database, date_start, date_end, print_statements=False):\n",
    "    \"\"\"\n",
    "    Function for 24 hour bin analysis of traffic incident data\n",
    "    df: Insert a Pandas dataframe\n",
    "    database: 'MMDA' or 'WAZE'\n",
    "    date format for date inputs: mm.dd.yyyy\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # Check database format\n",
    "    if database == 'MMDA':\n",
    "        databaseOutput = 'mmda_hourly'\n",
    "    elif database == 'WAZE':\n",
    "        databaseOutput = 'waze_hourly'\n",
    "    else:\n",
    "        raise ValueError(f'Invalid database argument: {database}')\n",
    "    \n",
    "    # Clean output folder\n",
    "    folder = 'data\\\\' + databaseOutput\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    dateEnd = date_end\n",
    "    dateStart = date_start\n",
    "    # Get variables for the end dates\n",
    "    dateEndMonth = int(dateEnd.split('.')[0])\n",
    "    dateEndDay = int(dateEnd.split('.')[1])\n",
    "    dateEndYear = int(dateEnd.split('.')[2])\n",
    "    # Get variables for the start dates\n",
    "    dateMonth = int(dateStart.split('.')[0])\n",
    "    dateDay = int(dateStart.split('.')[1])\n",
    "    dateYear = int(dateStart.split('.')[2])\n",
    "\n",
    "    # Variables for loop\n",
    "    runLoop = True\n",
    "    if print_statements == True:\n",
    "        print('=================================')\n",
    "        print(f'START {database} ANALYSIS')\n",
    "        print('=================================')\n",
    "    \n",
    "    while runLoop == True:\n",
    "        \n",
    "        df_selection = df[((df['Timestamp'].dt.month == dateMonth) & (df['Timestamp'].dt.day == dateDay))]\n",
    "        if print_statements == True:\n",
    "            print(f'Current Process: {dateMonth}.{dateDay}.{dateYear}')\n",
    "        \n",
    "        # Create filename for analysis in GIS software\n",
    "        # Add 0 padding to day numbers with single digit\n",
    "        if len(str(dateDay)) == 1:\n",
    "            dateDayFile = '0' + str(dateDay)\n",
    "        else:\n",
    "            dateDayFile = str(dateDay)\n",
    "        if len(str(dateMonth)) == 1:\n",
    "            dateMonthFile = '0' + str(dateMonth)\n",
    "        else:\n",
    "            dateMonthFile = dateMonth\n",
    "            \n",
    "        # Create appropriate filename, then save it\n",
    "        dfFilename = (str(dateYear) + str(dateMonth) + str(dateDayFile) + '_' \n",
    "                      + str(0) + str(24) + '.csv')\n",
    "        df_selection.to_csv('data\\\\' + databaseOutput + '\\\\' + dfFilename, index=False)\n",
    "\n",
    "        # Check if there are still days in the month. If not, proceed to next month\n",
    "        if int(dateDay) > calendar.monthrange(dateYear, dateMonth)[1]:\n",
    "            dateDay = 1\n",
    "            dateMonth += 1\n",
    "            if print_statements == True:\n",
    "                print('--- NEW MONTH START')\n",
    "        else:            \n",
    "            dateDay += 1\n",
    "\n",
    "        if ((dateMonth == dateEndMonth) & (dateDay == dateEndDay) & (dateYear == dateEndYear)):\n",
    "            break\n",
    "    \n",
    "\n",
    "    return print(f'{database} analysis done!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply function on Waze and MMDA Databases\n",
    "2 separate cells. Cell directly below is only used only when using a 24 hour bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMDA analysis done!\n",
      "\n",
      "WAZE analysis done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 24 HOUR BINS\n",
    "\n",
    "## DRIVER\n",
    "# df = pd.read_csv('DRIVER Accident Details (2).csv')\n",
    "# df['Timestamp'] = pd.to_datetime(df['Date'])\n",
    "# split_by_date_24hourbin(df=df, database='MMDA', date_start='10.31.2018', date_end='12.1.2018')\n",
    "\n",
    "df = pd.read_csv('mmda_data_edited.csv')\n",
    "df['Timestamp'] = df['Date'] + ' ' + df['Time']\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "split_by_date_24hourbin(df=df, database='MMDA', date_start='10.31.2018', date_end='12.1.2018')\n",
    "\n",
    "df = pd.read_csv('waze_feed.csv')\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "split_by_date_24hourbin(df=df, database='WAZE', date_start='10.31.2018', date_end='12.1.2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-24 hour bin below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMDA analysis done!\n",
      "\n",
      "WAZE analysis done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tweet2Map Database\n",
    "df = pd.read_csv('mmda_data_edited.csv')\n",
    "df['Timestamp'] = df['Date'] + ' ' + df['Time']\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "split_by_date(df=df, database='MMDA', date_start='10.31.2018', date_end='12.1.2018', bin_hour_size=3)\n",
    "\n",
    "# # DRIVER Database\n",
    "# df = pd.read_csv('DRIVER Accident Details (2).csv')\n",
    "# df['Timestamp'] = pd.to_datetime(df['Date'])\n",
    "# split_by_date(df=df, database='MMDA', date_start='10.31.2018', date_end='12.1.2018', bin_hour_size=3)\n",
    "\n",
    "# Waze database\n",
    "df = pd.read_csv('waze_feed.csv')\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "split_by_date(df=df, database='WAZE', date_start='10.31.2018', date_end='12.1.2018', bin_hour_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating a table to compare the databases\n",
    "Then this process will merge the 2 datasets by creating 2 columns to link the datasets. The initial for loops will read all the file paths of all the files in the directory, store the unique file paths in a dictionary, then save the dictionary items to a .txt file.\n",
    "\n",
    "Then it will generate a new .CSV file with 2 columns. One column of MMDA file paths and one column of Waze file paths. Each row will contain the file path of the waze/mmmda data and it will reference the same hour of the same day. This will prepare it for analysis using ArcGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database size: 683\n"
     ]
    }
   ],
   "source": [
    "dataMMDA = {}\n",
    "databaseMMDAFile = r'C:\\Users\\Panji\\Documents\\Python Scripts\\Non-Jupyter Py Scripts\\DOTr\\data\\mmda_hourly\\00_dictionary_mmda.txt'\n",
    "databaseMMDAPath = 'C:\\\\Users\\\\Panji\\\\Documents\\\\Python Scripts\\\\Non-Jupyter Py Scripts\\\\DOTr\\\\data\\\\mmda_hourly\\\\'\n",
    "databaseMMDA = open(databaseMMDAFile, 'w')\n",
    "dataWaze = {}\n",
    "databaseWazeFile = r'C:\\Users\\Panji\\Documents\\Python Scripts\\Non-Jupyter Py Scripts\\DOTr\\data\\waze_hourly\\00_dictionary_waze.txt'\n",
    "databaseWazePath = 'C:\\\\Users\\\\Panji\\\\Documents\\\\Python Scripts\\\\Non-Jupyter Py Scripts\\\\DOTr\\\\data\\\\waze_hourly\\\\'\n",
    "databaseWaze = open(databaseWazeFile, 'w')\n",
    "dataCombined = []\n",
    "\n",
    "# Waze Parser\n",
    "# Read Waze CSV files and save the file path to a dictionary\n",
    "\n",
    "\n",
    "for idx, file in enumerate(os.listdir(databaseWazePath)):\n",
    "    if '.txt' not in file:\n",
    "        dataWaze[idx] = file\n",
    "\n",
    "for x, y in dataWaze.items():\n",
    "    databaseWaze.writelines(str(x) + '/' + databaseWazePath + y + '\\n')\n",
    "databaseWaze.close()\n",
    "\n",
    "# MMDA Parser\n",
    "# Read MMDA CSV files and save the file path to a dictionary\n",
    "for idx, file in enumerate(os.listdir(databaseMMDAPath)):\n",
    "    if '.txt' not in file:\n",
    "        dataMMDA[idx] = file\n",
    "\n",
    "for x, y in dataMMDA.items():\n",
    "    databaseMMDA.writelines(str(x) + '/' + databaseMMDAPath + y + '\\n')\n",
    "databaseMMDA.close()\n",
    "\n",
    "# Merger process below\n",
    "# list organization of dataJoin:\n",
    "# item 0, Waze path\n",
    "# item 1, MMDA path\n",
    "for idxWaze, fileWaze in dataWaze.items():\n",
    "\n",
    "    # print('idxWaze: {}'.format(idxWaze))\n",
    "\n",
    "    with open(databaseMMDAFile) as f:\n",
    "        contentMMDA = f.readlines()\n",
    "\n",
    "    for x in contentMMDA:\n",
    "        if str(idxWaze) == str(x.split('/')[0]):\n",
    "            fileMMDA = x.split('/')[1]\n",
    "            fileMMDA = fileMMDA.replace('\\n', '')\n",
    "            dataJoin = [databaseWazePath + fileWaze, fileMMDA]\n",
    "            dataCombined.append(dataJoin)\n",
    "            break\n",
    "\n",
    "print('Database size: {}'.format(len(dataCombined)))\n",
    "\n",
    "for x in dataCombined:\n",
    "    wazeData = x[0]\n",
    "    MMDAData = x[1]\n",
    "\n",
    "df = pd.DataFrame(dataCombined,columns=['Waze','MMDA'])\n",
    "df.to_csv('combined_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: ArcGIS Buffer Analysis\n",
    "**NOTE: YOU NEED THE ARCGIS ARCPY MODULE WHICH ONLY COMES WITH ARCGIS SOFTWARE**\n",
    "\n",
    "This step will utilize the ArcPy package from ArcGIS to script and automate this buffer analysis.\n",
    "\n",
    "The for loop will read the CSV file row by row. In each row it will extract the MMDA and Waze data and create a buffer which is specified in the `bufferDistance` string. During each loop, matching data will be appended written to a separate pandas dataframe which is stored in `df_selection` and then appended to `df_match` which is the variable that keeps track of total matches. The dataframe in `df_match` is then saved to a .csv file. This part is for creating a histogram of matching data.\n",
    "\n",
    "An intersect analysis based and count the amount of MMDA point data that intersect with the Waze buffer. The output is not saved to any file but it is displayed as `print` statements and can be copied and pasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# SCRIPT PARAMETERS\n",
    "bufferDistance = \"300 Meters\"\n",
    "runMatchRate = False\n",
    "runIntersectionAnalysis = True\n",
    "\n",
    "# Declare initial variables\n",
    "scratch = 'C:\\\\Users\\\\Panji\\\\Documents\\\\Python Scripts\\\\Non-Jupyter Py Scripts\\\\DOTr\\\\scratch'\n",
    "scratchExcel = 'C:\\\\Users\\\\Panji\\\\Documents\\\\Python Scripts\\\\Non-Jupyter Py Scripts\\\\DOTr\\\\scratch\\\\scratchCsv.xls'\n",
    "df = pd.read_csv('combined_table.csv')\n",
    "spRef = arcpy.SpatialReference(\"C:\\\\GIS\\\\Data Files\\\\Work Files\\\\MMDA Tweet2Map\\\\input\\\\GCS_WGS_1984.prj\")\n",
    "outputWaze = 'outputWaze'\n",
    "outputMMDA = 'outputMMDA'\n",
    "BufferWaze_shp = 'C:\\\\Users\\\\Panji\\\\Documents\\\\Python Scripts\\\\Non-Jupyter Py Scripts\\\\DOTr\\\\scratch\\\\BufferWaze.shp'\n",
    "BufferMMDA_shp = 'C:\\\\Users\\\\Panji\\\\Documents\\\\Python Scripts\\\\Non-Jupyter Py Scripts\\\\DOTr\\\\scratch\\\\BufferMMDA.shp'\n",
    "Buffer_Intersect_shp = 'C:\\\\Users\\\\Panji\\\\Documents\\\\Python Scripts\\\\Non-Jupyter Py Scripts\\\\DOTr\\\\scratch\\\\intersection.shp'\n",
    "dataJoin = 'C:\\\\GIS\\\\Data Files\\\\Work Files\\\\scratch\\\\dotr_waze.gdb\\\\dataJoin'\n",
    "dataMatches = 'C:\\\\Users\\\\Panji\\\\Documents\\\\Python Scripts\\\\Non-Jupyter Py Scripts\\\\DOTr\\\\data\\\\matches'\n",
    "wazeTotal = 0\n",
    "MMDATotal = 0\n",
    "MMDADuplicateTotal = 0\n",
    "duplicateTotal = 0\n",
    "firstLoop = True\n",
    "\n",
    "# Process: Set workspace environment\n",
    "print('Set workspace environment')\n",
    "arcpy.env.workspace = r'C:\\Users\\Panji\\Documents\\Python Scripts\\Non-Jupyter Py Scripts\\DOTr\\scratch'\n",
    "print('Set output coordinate system')\n",
    "arcpy.env.outputCoordinateSystem = spRef\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "for idxMMDA, csvMMDA in enumerate(df['MMDA']):\n",
    "\n",
    "    print('===============================================')\n",
    "\n",
    "    # Get file from both MMDA and Waze\n",
    "    for idxWaze, csvWaze in enumerate(df['Waze']):\n",
    "\n",
    "        if (idxMMDA == idxWaze) and '.txt' not in csvWaze:\n",
    "            fileMMDA = csvMMDA\n",
    "            fileWaze = csvWaze\n",
    "            break\n",
    "        \n",
    "    print('Waze: ' + fileWaze)\n",
    "    print('MMDA: ' + fileMMDA)\n",
    "    \n",
    "    # print('Table to Table')\n",
    "    arcpy.TableToTable_conversion(fileWaze, scratch, \"tableWaze.dbf\")\n",
    "    arcpy.TableToTable_conversion(fileMMDA, scratch, \"tableMMDA.dbf\")\n",
    "\n",
    "    # print('Make XY Event Layer')\n",
    "    try:\n",
    "        arcpy.MakeXYEventLayer_management(scratch + '\\\\tableWaze.dbf', \"longitude\", \"latitude\", outputWaze, spRef)\n",
    "        arcpy.MakeXYEventLayer_management(scratch + '\\\\tableMMDA.dbf', \"longitude\", \"latitude\", outputMMDA, spRef)\n",
    "    except:\n",
    "        print('No data detected. Skipping set')\n",
    "        continue\n",
    "    \n",
    "    # print('Buffer Analysis')\n",
    "    arcpy.Buffer_analysis(outputWaze, BufferWaze_shp, bufferDistance, \"FULL\", \"ROUND\", \"ALL\", \"\", \"PLANAR\")\n",
    "    arcpy.Buffer_analysis(outputMMDA, BufferMMDA_shp, bufferDistance, \"FULL\", \"ROUND\", \"ALL\", \"\", \"PLANAR\")\n",
    "\n",
    "    # print('Intersect Analysis')\n",
    "    arcpy.Intersect_analysis([BufferWaze_shp, outputMMDA], Buffer_Intersect_shp, \"ALL\", \"\", \"POINT\")\n",
    "    \n",
    "    if runMatchRate == True:\n",
    "        # Keep track of the matches in a pandas dataframe\n",
    "        # Get the initial dataframe running\n",
    "        print('runMatchRate: {}'.format(runMatchRate))\n",
    "        arcpy.TableToExcel_conversion(Buffer_Intersect_shp, scratchExcel)\n",
    "        \n",
    "        if firstLoop == True:\n",
    "            df_match = pd.read_excel(scratchExcel)\n",
    "            df_match = df_match[0:0]\n",
    "            firstLoop = False\n",
    "        \n",
    "        df_selection = pd.read_excel(scratchExcel)\n",
    "        df_match = df_match.append(df_selection)\n",
    "        \n",
    "    if runIntersectionAnalysis == True:\n",
    "        # Keep track of the Waze data that matches to MMDA data\n",
    "        # Waze point data inside MMDA buffer\n",
    "        # Extract time and date to create a unique Excel file\n",
    "        print('runIntersectionAnalysis: {}'.format(runIntersectionAnalysis))\n",
    "        arcpy.SpatialJoin_analysis(BufferMMDA, outputWaze, wazeMatches, 'JOIN_ONE_TO_MANY', match_option='INTERSECT')\n",
    "        fileWazeMatches = fileWaze.split('\\\\')[-1]\n",
    "        fileWazeMatches = fileWazeMatches.split('.')[0]\n",
    "        arcpy.TableToExcel_conversion(wazeMatches, dataMatches + fileWazeMatches + '.xls')\n",
    "\n",
    "    print('Output Statistics')\n",
    "    statIntersect = arcpy.GetCount_management(Buffer_Intersect_shp)\n",
    "    statIntersect = int(statIntersect.getOutput(0))\n",
    "    statWaze = arcpy.GetCount_management(outputWaze)\n",
    "    statWaze = int(statWaze.getOutput(0))\n",
    "    statMMDA = arcpy.GetCount_management(outputMMDA)\n",
    "    statMMDA = int(statMMDA.getOutput(0))\n",
    "\n",
    "     # Clean outputs\n",
    "    print('Reset variables')\n",
    "    arcpy.Delete_management(\"in_memory\")\n",
    "    arcpy.Delete_management('C:\\\\Users\\\\Panji\\\\Documents\\\\Python Scripts\\\\Non-Jupyter Py Scripts\\\\DOTr\\\\scratch\\\\tableWaze.dbf')\n",
    "    arcpy.Delete_management('C:\\\\Users\\\\Panji\\\\Documents\\\\Python Scripts\\\\Non-Jupyter Py Scripts\\\\DOTr\\\\scratch\\\\tableMMDA.dbf')\n",
    "\n",
    "    # Statistics\n",
    "    wazeTotal += statWaze\n",
    "    MMDATotal += statMMDA\n",
    "    duplicateTotal += statIntersect\n",
    "    duplicateRatio = float(float(duplicateTotal) / float(MMDATotal))\n",
    "    \n",
    "    print('\\n\\nCURRENT SET:')\n",
    "    print('Waze Incidents: {}'.format(statWaze))\n",
    "    print('MMDA Incidents: {}'.format(statMMDA))\n",
    "    print('Total duplicates: {}\\n\\n'.format(statIntersect))\n",
    "    print('TOTAL SET:')\n",
    "    print('Total Waze Incidents: {}'.format(wazeTotal))\n",
    "    print('Total MMDA Incidents: {}'.format(MMDATotal))\n",
    "    print('Total MMDA Duplicates: {}'.format(duplicateTotal))\n",
    "    print('Duplicate/MMDA Total: {}/{}\\n'.format(duplicateTotal, MMDATotal))\n",
    "    print('Current MMDA Duplicate Ratio: {:.2f}'.format(duplicateRatio))\n",
    "    print('Current matching dataframe shape: {}'.format(df_match.shape))\n",
    "\n",
    "df_match.to_csv(r'C:\\Users\\Panji\\Documents\\Python Scripts\\Non-Jupyter Py Scripts\\DOTr\\data\\histograms\\histogram.csv',\n",
    "                index=False, encoding='utf-8')\n",
    "print('===============================')\n",
    "print('\\nAnalysis done!')\n",
    "print('Duplicate/MMDA Total: {}/{}'.format(duplicateTotal, MMDATotal))\n",
    "print('Current MMDA Duplicate Ratio: {:.2f}\\n'.format(duplicateRatio))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
